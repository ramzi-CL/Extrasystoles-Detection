{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "f2f1c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from constants import *\n",
    "from preprocessing_utils import *\n",
    "from hots_utils import *\n",
    "import neurokit2 as nk\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "bf6ae91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_peaks_annotations(row, ann_df):\n",
    "    peaks = row['r_peaks']\n",
    "    filename = row['file']\n",
    "    signal_extrasystoles = ann_df.loc[ann_df['file']==filename]\n",
    "    annotation = []\n",
    "    \n",
    "    n_extrasystoles = signal_extrasystoles.shape[0]\n",
    "    n_peaks_affected_to_extrasystoles = 0\n",
    "    extrasystoles_affected_index = []\n",
    "    \n",
    "    for peak in peaks:\n",
    "        peak_annotation_found_in_extrasystoles = False\n",
    "        for i in range(signal_extrasystoles.shape[0]):\n",
    "            extrasystole = signal_extrasystoles.iloc[i]\n",
    "            if (peak >= extrasystole.start) and (peak <= extrasystole.end):\n",
    "                annotation.append(extrasystole.type)\n",
    "                \n",
    "                peak_annotation_found_in_extrasystoles = True\n",
    "                n_peaks_affected_to_extrasystoles+=1\n",
    "                extrasystoles_affected_index.append(i)\n",
    "                \n",
    "                break\n",
    "        if peak_annotation_found_in_extrasystoles==False:\n",
    "            annotation.append('N')\n",
    "    if n_peaks_affected_to_extrasystoles!=n_extrasystoles:\n",
    "        print('file {} : there is {} extrasystole(s) not affected to peaks'\\\n",
    "              .format(filename, n_extrasystoles-n_peaks_affected_to_extrasystoles))\n",
    "        print('Total extrasystoles present in file :{}'.format(n_extrasystoles))\n",
    "        print('indexes of extrasystoles affected '+str(extrasystoles_affected_index))\n",
    "        print()\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def add_annotation(sig_df, ann_df):\n",
    "    ann_sig_df = sig_df.copy()\n",
    "    ann_sig_df['annotation'] = ann_sig_df.apply(lambda x: generate_peaks_annotations(x, ann_df), axis=1)\n",
    "    return ann_sig_df\n",
    "\n",
    "\n",
    "def binarize_annotation_list(row):\n",
    "    ann_list = row['annotation']\n",
    "    binary_ann_list = [0 if ann=='N' else 1 for ann in ann_list]\n",
    "    return binary_ann_list\n",
    "\n",
    "def binarize_annotation(data):\n",
    "    data_ = data.copy()\n",
    "    data_['annotation'] = data_.apply(lambda x: binarize_annotation_list(x), axis=1)\n",
    "    return data_\n",
    "\n",
    "def compute_rr(i, samples):\n",
    "    previous_sample = samples[i-1]\n",
    "    current_sample = samples[i]\n",
    "    next_sample = samples[i+1]\n",
    "    rr_1 = current_sample - previous_sample\n",
    "    rr_2 = next_sample - current_sample\n",
    "    rr_1_per = rr_1 / (rr_1 + rr_2)\n",
    "    rr_2_per = rr_2 / (rr_1 + rr_2)\n",
    "    return rr_1_per, rr_2_per\n",
    "\n",
    "def determine_annotation(i, ann, normal_beats, extrasystole_beats):\n",
    "    annotation = -1\n",
    "    if (ann[i] in extrasystole_beats) or (ann[i+1] in extrasystole_beats):\n",
    "        annotation = 1\n",
    "    elif (ann[i] in normal_beats) and (ann[i+1] in normal_beats) and \\\n",
    "            (ann[i-1] in normal_beats):\n",
    "        annotation = 0\n",
    "    return annotation\n",
    "\n",
    "\n",
    "def split_signal(row, normal_beats, extrasystole_beats, fs):\n",
    "    dataframe = pd.DataFrame()\n",
    "    record_name = row['file']\n",
    "    signal = row['signal']\n",
    "    ann = row['annotation']\n",
    "    samples = list(row['r_peaks'])\n",
    "    allowed_beats = normal_beats + extrasystole_beats\n",
    "    for i in range(1, len(ann)-2):\n",
    "        anns_seq = [ann[i-1], ann[i], ann[i+1], ann[i+2]]\n",
    "        if all(e in allowed_beats for e in anns_seq):\n",
    "            template_signal = extract_template_signal(i, samples, signal, fs)\n",
    "            rr_1, rr_2 = compute_rr(i, samples)\n",
    "            annotation = determine_annotation(\n",
    "                i, ann, normal_beats, extrasystole_beats)\n",
    "            if annotation in [0, 1]:\n",
    "                dataframe = dataframe.append({'record_name': record_name,\n",
    "                                              'template': template_signal,\n",
    "                                              'rr_1': rr_1,\n",
    "                                              'rr_2': rr_2,\n",
    "                                              'label': annotation},\n",
    "                                             ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "def generate_templates(data, normal_beats, extrasystole_beats, fs):\n",
    "    templates_df = pd.DataFrame()\n",
    "    list_of_df = list(data.apply(\n",
    "        lambda x: split_signal(x, normal_beats, extrasystole_beats, fs),\n",
    "        axis=1))\n",
    "    for df in list_of_df:\n",
    "        templates_df = pd.concat([templates_df, df], ignore_index=True)\n",
    "    return templates_df\n",
    "\n",
    "\n",
    "def scale_template(row):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    signal = np.array(row['template'])\n",
    "    scaled_signal = scaler.fit_transform(signal.reshape(-1, 1)).flatten()\n",
    "    return scaled_signal\n",
    "\n",
    "\n",
    "def minmax_scale(data):\n",
    "    data_ = data.copy()\n",
    "    data_['template'] = data_.apply(lambda x: scale_template(x), axis=1)\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "c3240ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read annotations from xls file\n",
    "annotation_df = pd.read_excel('Marc Annotation/annotation/annotations.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "5f97a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect R peaks\n",
    "signal_df = pd.DataFrame()\n",
    "for ecg_file in ecg_files:\n",
    "    ecg_folder = ecg_file.split('_')[0]\n",
    "    ecg = list(pd.read_csv('Marc annotation/signaux/'+ecg_folder+'/'+ecg_file).value)\n",
    "    _, r_peaks = nk.ecg_peaks(pd.Series(ecg, name='ECG'), sampling_rate=CHRONOLIFE_FS)\n",
    "    r_peaks = r_peaks['ECG_R_Peaks']\n",
    "    signal_df = signal_df.append({'file': ecg_file, 'signal': ecg, 'r_peaks': r_peaks}, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "cfbf442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate signal beat annotation vector based on Marc annotation\n",
    "annotated_signal_df = add_annotation(signal_df, annotation_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "e7033696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confuse ventricular and atrial extrasystoles\n",
    "annotated_signal_df = binarize_annotation(annotated_signal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "2cd60d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate templates and their annotations\n",
    "templates_df = generate_templates(data=annotated_signal_df, \n",
    "                                  normal_beats=[0], \n",
    "                                  extrasystole_beats=[1], \n",
    "                                  fs=CHRONOLIFE_FS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "ad2ef30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale templates\n",
    "templates_df = minmax_scale(templates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "f35459d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load centers\n",
    "with open('centers.pkl','rb') as f:\n",
    "    centers = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "ab0c9f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RamziAbdelhafidh\\Documents\\Chronolife\\MIT-BIH Arrhythmia\\pylife-dev\\pylife-dev\\pylife\\machine_learning\\old_hots_functions.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context = np.array(context)\n",
      "C:\\Users\\RamziAbdelhafidh\\Documents\\Chronolife\\MIT-BIH Arrhythmia\\pylife-dev\\pylife-dev\\pylife\\machine_learning\\old_hots_functions.py:56: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  context = np.array(context)\n"
     ]
    }
   ],
   "source": [
    "# generate hots features\n",
    "#templates_df = generate_hots_test_features(templates_df, centers, **CHR_HOTS_PARAMS)\n",
    "templates_df = generate_hots_test_features(templates_df.groupby('label').sample(148), centers, **CHR_HOTS_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "dccdbb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load classifier\n",
    "with open('classifier.pkl','rb') as f:\n",
    "    clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "fbeba4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.99      0.95       148\n",
      "         1.0       0.99      0.92      0.95       148\n",
      "\n",
      "    accuracy                           0.95       296\n",
      "   macro avg       0.95      0.95      0.95       296\n",
      "weighted avg       0.95      0.95      0.95       296\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[146,   2],\n",
       "       [ 12, 136]], dtype=int64)"
      ]
     },
     "execution_count": 626,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = templates_df.drop(columns=['record_name', 'template', 'label'], axis=1)\n",
    "y_test = templates_df['label']\n",
    "y_pred = clf.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
